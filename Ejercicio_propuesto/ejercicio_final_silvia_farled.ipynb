{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd26277b",
   "metadata": {},
   "source": [
    "#### FASE 0: CONFIGURACI√ìN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629b49d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librer√≠as\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "# Imputaci√≥n de nulos usando m√©todos avanzados estad√≠sticos\n",
    "# -----------------------------------------------------------------------\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "\n",
    "# Librer√≠as de visualizaci√≥n\n",
    "# -----------------------------------------------------------------------\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Evaluar linealidad de las relaciones entre las variables\n",
    "# ------------------------------------------------------------------------------\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import shapiro, poisson, chisquare, expon, kstest\n",
    "from scipy.stats import levene, bartlett, shapiro\n",
    "\n",
    "# Gesti√≥n de los warnings\n",
    "# -----------------------------------------------------------------------\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # Para evitar errores en el uso de palette en seaborn\n",
    "\n",
    "\n",
    "# ver todas las columnas\n",
    "pd.set_option('display.max_columns', None)\n",
    "# ver todas las filas\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1908c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n EDA\n",
    "\n",
    "def basic_eda(df):\n",
    "\n",
    "    print('üå∑Ejemplo de datos del DF:')\n",
    "    display(df.head(3))\n",
    "    display(df.tail(3))\n",
    "    display(df.sample(3))\n",
    "    print('________________________________________________________________________________________________________')\n",
    "\n",
    "    print('üåªN√∫mero de Filas:')\n",
    "    display(df.shape[0])\n",
    "    print('________________________________________________________________________________________________________')\n",
    "\n",
    "    print('üå±N√∫mero de Columnas:')\n",
    "    display(df.shape[1])\n",
    "    print('________________________________________________________________________________________________________')\n",
    "\n",
    "    print('üåºInformaci√≥n de la tabla:')\n",
    "    display(df.info())\n",
    "    print('________________________________________________________________________________________________________')\n",
    "\n",
    "    print('üåëNombre de las columnas:')\n",
    "    display(df.columns)\n",
    "    print('________________________________________________________________________________________________________')\n",
    "\n",
    "    print('üçÑDescripci√≥n de los datos num√©ricos:')\n",
    "    display(df.describe().T)\n",
    "    print('________________________________________________________________________________________________________')\n",
    "\n",
    "    print('üåãDescripci√≥n de los datos no-num√©ricos:')\n",
    "    try:\n",
    "        display(df.describe(include='object').T)\n",
    "    except:\n",
    "        pass\n",
    "    print('________________________________________________________________________________________________________')\n",
    "\n",
    "    print('üçÇSaber si hay datos √∫nicos:')\n",
    "    display(df.nunique())\n",
    "    print('________________________________________________________________________________________________________')\n",
    "\n",
    "    print('üêñQue datos son nulos por columnas:')\n",
    "    display(df.isnull().sum())\n",
    "    print('________________________________________________________________________________________________________')\n",
    "\n",
    "    print('üê≤Filas duplicadas:')\n",
    "    total_duplicados = df.duplicated().sum()\n",
    "    if total_duplicados > 0:\n",
    "        print(f'cantidad de duplicados: {total_duplicados}')\n",
    "        print('Primeros duplicados')\n",
    "        display(df[df.duplicated()].head(3))\n",
    "    else:\n",
    "        print('No hay duplicados')\n",
    "    print('________________________________________________________________________________________________________')\n",
    "\n",
    "    print('ü™π Columnas constantes (solo 1 valor √∫nico):')\n",
    "    constantes = df.columns[df.nunique() <= 1]\n",
    "    if len(constantes) > 0:\n",
    "        print(f'{len(constantes)} columnas con 1 valor √∫nico:')\n",
    "        display(constantes)\n",
    "    else:\n",
    "        print('No hay columnas constantes')\n",
    "    print('________________________________________________________________________________________________________')\n",
    "    \n",
    "    print('üöÄ Valores √∫nicos en columnas categ√≥ricas:')\n",
    "    for col in df.select_dtypes(include='object'):\n",
    "        print(f'üî∏ {col}')\n",
    "        print('-----------------------------')\n",
    "        print(df[col].unique())\n",
    "        print('________________________________________________________________________________________________________')\n",
    "\n",
    "    print('üß¨ Tipos de datos por columna:')\n",
    "    display(df.dtypes.value_counts())\n",
    "    print('________________________________________________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e97d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n uni√≥n de tablas con el m√©todo \"right\"\n",
    "\n",
    "def to_union(df1,df2):\n",
    "    df_new = df1.merge(df2, how='right')\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a833898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para localizar y eliminar los registros duplicados\n",
    "\n",
    "def duplicates(df):\n",
    "    if df.duplicated().sum() > 0:\n",
    "        df= df.drop_duplicates()\n",
    "        return df\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7b3bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para cambiar el tipo de dato de una columna seg√∫n necesidad\n",
    "\n",
    "def change_data(df, column, type):\n",
    "    try:\n",
    "        if type == \"int\":\n",
    "            df[column] = df[column].astype(int)\n",
    "        elif type == \"object\":\n",
    "            df[column] = df[column].astype(str)\n",
    "        elif type == \"datetime\":\n",
    "            df[column] = pd.to_datetime(df[column], errors='coerce')\n",
    "        elif type == \"float\":\n",
    "            df[column] = df[column].astype(float)\n",
    "        else:\n",
    "            print(\"Solo se acepta: int, object, datetime o float\")\n",
    "    except:\n",
    "        print(\"Hay nulos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aaa72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para concatenar las fechas en una sola columna y eliminar las originales\n",
    "\n",
    "def date_union (df, year_column, month_column, date_column): \n",
    "    localizacion = df.columns.get_loc(year_column)\n",
    "    df.insert(localizacion, date_column, None)\n",
    "    df[date_column] = df[year_column].astype(str) + \"-\" + df[month_column].astype(str).str.zfill(2) + \"-01\"\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "    df[date_column] = df[date_column].dt.date\n",
    "    df.drop([year_column, month_column], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8a647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para realizar una prueba de hip√≥tesis con m√∫ltiples grupos\n",
    "\n",
    "def multigroup_hypothesis_test(*args):\n",
    "\n",
    "    \"\"\"\n",
    "    Realiza una prueba de hip√≥tesis para comparar grupos.\n",
    "    1. Primero verifica si los datos son normales usando el test de Shapiro-Wilk o Kolmogorov-Smirnov.\n",
    "    2. Si los datos son normales, usa Bartlett para probar igualdad de varianzas. Si no son normales, usa Levene.\n",
    "    3. Si las varianzas son iguales, usa el ANOVA; si no, usa la versi√≥n de Kruskal-Wallis.\n",
    "\n",
    "    Par√°metros:\n",
    "    *args: listas o arrays con los datos de cada grupo. Espera VARIOS grupos a comparar\n",
    "\n",
    "    Retorna:\n",
    "    dict con resultados del test de normalidad, varianza e hip√≥tesis.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if len(args) < 2:\n",
    "        raise ValueError(\"Se necesitan al menos dos conjuntos de datos para realizar la prueba.\")\n",
    "\n",
    "    # Test de normalidad por grupo\n",
    "    normality = []\n",
    "    for group in args:\n",
    "        if len(group) > 50:\n",
    "            p_value_norm = stats.kstest(group, 'norm').pvalue\n",
    "        else:\n",
    "            p_value_norm = stats.shapiro(group).pvalue\n",
    "        normality.append(p_value_norm > 0.05)\n",
    "\n",
    "    normal_data = all(normality)\n",
    "\n",
    "    # Test de igualdad de varianzas para todos los grupos juntos\n",
    "    if normal_data:\n",
    "        p_variance_value = stats.bartlett(*args).pvalue\n",
    "    else:\n",
    "        p_variance_value = stats.levene(*args, center=\"median\").pvalue\n",
    "\n",
    "    equal_variances = p_variance_value > 0.05\n",
    "\n",
    "    # Test final seg√∫n normalidad y varianzas\n",
    "    if normal_data and equal_variances:\n",
    "        t_stat, p_value = stats.f_oneway(*args)\n",
    "        test_used = \"ANOVA\"\n",
    "    else:\n",
    "        t_stat, p_value = stats.kruskal(*args)\n",
    "        test_used = \"Kruskal-Wallis\"\n",
    "\n",
    "    alfa = 0.05\n",
    "\n",
    "    result = {\n",
    "        \"Test de Normalidad\": normality,\n",
    "        \"Datos Normales\": normal_data,\n",
    "        \"p-valor Varianza\": p_variance_value,\n",
    "        \"Varianzas Iguales\": equal_variances,\n",
    "        \"Test Usado\": test_used,\n",
    "        \"Estad√≠stico\": t_stat,\n",
    "        \"p-valor\": p_value,\n",
    "        \"Conclusi√≥n\": \"Rechazamos H0. Hay diferencias significativas entre grupos\" if p_value < alfa else \"No se rechaza H0. No hay diferencias significativas entre grupos\"\n",
    "    }\n",
    "\n",
    "    print(\"\\nüìä **Resultados de la Prueba de Hip√≥tesis Multigrupos** üìä\")\n",
    "    print(f\"‚úÖ Normalidad en todos los grupos: {'S√≠' if normal_data else 'No'}\")\n",
    "    print(f\"   - Normalidad por grupo: {normality}\")\n",
    "    print(f\"‚úÖ Varianzas: {'Iguales' if equal_variances else 'Desiguales'} (p = {p_variance_value:.4f})\")\n",
    "    print(f\"‚úÖ Test aplicado: {test_used}\")\n",
    "    print(f\"üìâ Estad√≠stico: {t_stat:.4f}, p-valor: {p_value:.4f}\")\n",
    "    print(f\"üîç Conclusi√≥n: {result['Conclusi√≥n']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a972b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecemos paleta de color para los gr√°ficos \n",
    "\n",
    "pastel_colors = [\n",
    "    \"#A8DADC\",  # Verde azulado pastel (fr√≠o)\n",
    "    \"#FFE5B4\",  # Amarillo pastel (c√°lido)\n",
    "    \"#F1C0E8\",  # Lila suave (fr√≠o)\n",
    "    \"#FFD6D6\",  # Rosa melocot√≥n muy suave (c√°lido)\n",
    "    \"#B5EAEA\",  # Turquesa pastel (fr√≠o)\n",
    "    \"#FFCBC1\",  # Salm√≥n pastel (c√°lido)\n",
    "    \"#C1E1C1\",  # Verde menta claro (fr√≠o)\n",
    "    \"#FFE3E3\",  # Rosa claro (c√°lido)\n",
    "    \"#D4A5A5\",  # Marr√≥n claro pastel (c√°lido)\n",
    "    \"#C6D8D3\",  # Verde gris√°ceo pastel (fr√≠o)\n",
    "    \"#F7E6C4\",  # Beige claro (c√°lido)\n",
    "    \"#B9AEDC\",  # Lavanda pastel (fr√≠o)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db91f2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuramos valores para el formato de las tablas por defecto:\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Comic Sans MS',\n",
    "    'axes.titlesize': 18,\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.labelsize': 14,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'figure.facecolor' : 'lightgray',\n",
    "    'axes.facecolor': 'darkgray'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9f8e07",
   "metadata": {},
   "source": [
    "#### FASE 1: EXPLORACI√ìN Y LIMPIEZA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa66ffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrimos el archivo 1\n",
    "\n",
    "df_fligh = pd.read_csv(\"Customer Flight Activity.csv\")\n",
    "df_fligh.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c8cefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrimos el archivo 2\n",
    "\n",
    "df_loyalty=pd.read_csv(\"Customer Loyalty History.csv\")\n",
    "df_loyalty.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d50a182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos la funci√≥n de EDA al archivo 1 para entender los datos\n",
    "\n",
    "basic_eda(df_fligh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f565eee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos la funci√≥n de EDA al archivo 2 para entender los datos\n",
    "\n",
    "basic_eda(df_loyalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aab37a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tras comprobar que hay filas duplicadas en el archivo 1 usamos la funci√≥n para eliminar duplicados\n",
    "\n",
    "df_fligh = duplicates(df_fligh)\n",
    "\n",
    "# Comprobamos tambi√©n que la informaci√≥n del archivo 1 continene los vuelos reservados de los clientes. Si la columna de vuelos totales es 0 no aporta valor. Eliminamos las filas cuya columna es 0:\n",
    "\n",
    "df_fligh = df_fligh[df_fligh[\"Total Flights\"] != 0]\n",
    "\n",
    "# Unimos las dos tablas en una sola por el m√©todo right teniendo en cuenta que el archivo 2 tiene solo un registro por cliente y el archivo 1 tiene varios registros por cliente_\n",
    "\n",
    "df = to_union(df_loyalty, df_fligh)\n",
    "\n",
    "# Practicamos un nuevo an√°lisis EDA sobre el archivo unificado: \n",
    "\n",
    "basic_eda(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f441d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos histogramas para valorar las columnas num√©ricas\n",
    "\n",
    "df.select_dtypes(include=[np.number]).hist(figsize=(16,12), bins=30, color = \"black\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f25cba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobamos que la columna Salary tiene valores negativos. Utilizamos .abs() para sacar el valor absoluto:\n",
    "\n",
    "df[\"Salary\"] = df[\"Salary\"].abs()\n",
    "\n",
    "# Tambi√©n tiene valores nulos. Elegimos el m√©todo KNN para la imputaci√≥n y facilitamos otras columnas num√©ricas en las que apoyar la imputaci√≥n: \n",
    "\n",
    "columns_to_imput = [\"Salary\", \"CLV\", \"Distance\", \"Points Accumulated\"]\n",
    "imputer_knn = KNNImputer(n_neighbors=5)\n",
    "df[columns_to_imput] = imputer_knn.fit_transform(df[columns_to_imput])\n",
    "\n",
    "# Completamos los nulos de las columnas de mes y a√±o de cancelaci√≥n a fechas absurdas en el futuro: \n",
    "\n",
    "df[\"Cancellation Year\"] = df[\"Cancellation Year\"].fillna(2262)\n",
    "df[\"Cancellation Month\"] = df[\"Cancellation Month\"].fillna(1)\n",
    "\n",
    "# Una vez gestionado los nulos, cambiamos el tipo de dato de estas columnas de float a int: \n",
    "\n",
    "columns_int = [\"Points Accumulated\", \"Salary\", \"Cancellation Year\", \"Cancellation Month\"]\n",
    "\n",
    "for column in columns_int: \n",
    "    change_data(df, column, \"int\")\n",
    "    print(f\"Cambiada la columna {column}\")\n",
    "\n",
    "# Concatenamos las fechas en una sola columna juntando el a√±o, el mes y el d√≠a 1. Lo convertimos a datatime y eliminamos las columnas de a√±o y mes iniciales con la funci√≥n date_union: \n",
    "\n",
    "date_union(df, \"Enrollment Year\", \"Enrollment Month\", \"Enrollment Date\")\n",
    "date_union(df, \"Cancellation Year\", \"Cancellation Month\", \"Cancellation Date\")\n",
    "date_union(df, \"Year\", \"Month\", \"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76506a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76c96d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos en un nuevo csv los datos limpios para empezar con los ejercicios. \n",
    "\n",
    "df.to_csv(\"Customer_union.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091728c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"Customer_union.csv\", index_col = 0)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57e34a8",
   "metadata": {},
   "source": [
    "#### FASE 2: VISUALIZACI√ìN  \n",
    "  \n",
    "- 1. ¬øC√≥mo se distribuye la cantidad de vuelos reservados por mes durante el a√±o?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2370fce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico para ejercicio 1\n",
    "\n",
    "df_booking = df.groupby(\"Date\")[\"Flights Booked\"].sum()\n",
    "df_booking.index = pd.to_datetime(df_booking.index)\n",
    "formatted_dates = df_booking.index.strftime(\"%B %Y\")  # Da formato a la columna para mostrar mes en letra\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x=formatted_dates, height=df_booking.values, color=pastel_colors, edgecolor='black')\n",
    "\n",
    "plt.xlabel('Meses')\n",
    "plt.ylabel('Vuelos Reservados')\n",
    "plt.title('Vuelos Reservados por Mes', fontsize=22)\n",
    "plt.xticks(rotation=45)\n",
    "ticks_y = np.arange(0, 110000, 10000)\n",
    "plt.yticks(ticks_y)\n",
    "plt.grid(True, axis='y', color='#eeeeee')\n",
    "plt.gca().spines[['right', \"top\"]].set_visible(False) # quitamos la l√≠nea de arriba y de la derecha\n",
    "plt.axvline(x=12 - 0.5, color='gray', linestyle='--', linewidth=1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527f93c3",
   "metadata": {},
   "source": [
    "Respuesta: tras poder comprobar la informaci√≥n por mes de dos a√±os consecutivos vemos que el patr√≥n se repite, siendo el mes con m√°s movimiento el de julio pero seguido de cerca por los meses de junio, agosto y diciembre. Podemos relacionar estos picos con las vacaciones estivales y Navidad. Por contra, los meses con menos movimiento son enero y febrero que coincide con que no hay fiestas rese√±ables en estos meses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e44e08b",
   "metadata": {},
   "source": [
    "- 2. ¬øExiste una relaci√≥n entre la distancia de los vuelos y los puntos acumulados por los cliente?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6315e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico para ejercicio 2\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (20, 5))\n",
    "\n",
    "sns.regplot(x = \"Distance\", \n",
    "            y = \"Points Accumulated\", \n",
    "            data = df, \n",
    "            marker = \"d\", \n",
    "            line_kws = {\"color\": \"black\", \"linewidth\": 1}, # cambiamos el color y el grosor de la linea de tendencia\n",
    "            scatter_kws = {\"color\": \"crimson\", \"s\": 2}, # cambiamos el color y el tama√±o de los puntos del scaterplot\n",
    "            ax = axes[0])\n",
    "axes[0].set_xlabel(\"Distancia\")\n",
    "axes[0].set_ylabel(\"Puntos Acumulados\")\n",
    "axes[0].set_title(\"Relaci√≥n distancia recorrida y puntos acumulados\")\n",
    "axes[0].spines[['top', 'right']].set_visible(False)\n",
    "\n",
    "df_corr = df[[\"Distance\", \"Points Accumulated\"]].corr( method = \"spearman\").round(2)\n",
    "df_corr.index = ['Distancia', 'Puntos']\n",
    "df_corr.columns = ['Distancia', 'Puntos']\n",
    "sns.heatmap(df_corr, \n",
    "            cmap='coolwarm', \n",
    "            annot=True, \n",
    "            fmt='.2f', \n",
    "            linewidths=0.5, \n",
    "            vmin=-1, \n",
    "            vmax=1, \n",
    "            ax = axes[1])\n",
    "axes[1].set_title(\"Matriz de Correlaci√≥n de distancia recorrida y puntos acumulados\", fontsize=14)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39000e04",
   "metadata": {},
   "source": [
    "Respuesta: definitivamente s√≠, est√°n estrechamente relacioneados. A m√°s distancia recorrida en vuelo m√°s puntos acumulan los clientes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b62c26",
   "metadata": {},
   "source": [
    "- 3. ¬øCu√°l es la distribuci√≥n de los clientes por provincia o estado?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f492c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico para ejercicio 3\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "sns.countplot(data=df, \n",
    "              x=\"Province\", \n",
    "              order=df[\"Province\"].value_counts().index, \n",
    "              palette=pastel_colors, \n",
    "              edgecolor='black', \n",
    "              width=0.6)\n",
    "plt.title('Distribuci√≥n de clientes por provincia')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Provincias')\n",
    "plt.ylabel('N√∫mero de clientes')\n",
    "plt.grid(True, axis='y', color='#eeeeee')\n",
    "plt.gca().spines[['right', \"top\"]].set_visible(False) # quitamos la l√≠nea de arriba y de la derecha\n",
    "plt.show()\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a91307",
   "metadata": {},
   "source": [
    "Respuesta: Esta es la distribuci√≥n, siendo con diferencia Ontario, British Columbia y Quebec las que acumulan la mayor parte de los clientes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6c504b",
   "metadata": {},
   "source": [
    "- 4. ¬øC√≥mo se compara el salario promedio entre los diferentes niveles educativos de los clientes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1bfb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico para ejercicio 4\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "df_salary = df.groupby(\"Education\")[\"Salary\"].mean()\n",
    "order_education = ['High School or Below', 'College', 'Bachelor', 'Master', 'Doctor']\n",
    "df_salary = df_salary.reindex(order_education)\n",
    "df_salary.index = ['Secundaria', 'T√©cnica', 'Grado', 'M√°ster', 'Doctorado']\n",
    "\n",
    "df_salary.plot(kind='bar', color = pastel_colors, edgecolor='black', width=0.4)\n",
    "plt.title('Salario promedio por nivel de estudios')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Nivel de educaci√≥n')\n",
    "plt.ylabel('Salario promedio')\n",
    "plt.grid(True, axis='y', color='#eeeeee')\n",
    "plt.gca().spines[['right', \"top\"]].set_visible(False) # quitamos la l√≠nea de arriba y de la derecha\n",
    "plt.show()\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fb466b",
   "metadata": {},
   "source": [
    "Respuesta: el salario aumenta en relaci√≥n al nivel de estudios con la √∫nica excepci√≥n de que los t√©cnicos cobran de media ligeramente m√°s que las personas con grado universitario. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de9d104",
   "metadata": {},
   "source": [
    "- 5. ¬øCu√°l es la proporci√≥n de clientes con diferentes tipos de tarjetas de fidelidad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997b389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico para ejercicio 5\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "\n",
    "df_fidelity = df[\"Loyalty Card\"].value_counts()\n",
    "plt.pie(df_fidelity.values, \n",
    "        labels= df_fidelity.index,\n",
    "        data = df, \n",
    "        autopct=  '%1.1f%%', \n",
    "        colors = pastel_colors, \n",
    "        textprops={'fontsize': 10}, \n",
    "        startangle=90, \n",
    "        wedgeprops={'edgecolor': 'black'} );\n",
    "plt.title('Proporci√≥n clientes por tarjeta de fidelidad',fontsize=18, fontname='Comic Sans MS')\n",
    "plt.axis('equal')  # Para que el c√≠rculo sea perfecto\n",
    "plt.show()\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ca2b1f",
   "metadata": {},
   "source": [
    "Respuesta: nada que a√±adir a lo reflejado en el gr√°fico:  \n",
    "  \n",
    "Star: 45.6%  \n",
    "Aurora: 20.6%  \n",
    "Nova: 33.8%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb9f657",
   "metadata": {},
   "source": [
    "- 6. ¬øC√≥mo se distribuyen los clientes seg√∫n su estado civil y g√©nero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ee1b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico para ejercicio 6\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "new_names = [\"Casado\", \"Soltero\", \"Divorciado\"]\n",
    "sns.countplot(x = \"Marital Status\", \n",
    "              data = df,\n",
    "              palette = pastel_colors,\n",
    "              width=0.4, \n",
    "              hue = \"Gender\", \n",
    "              edgecolor='black')\n",
    "plt.xlabel(\"Estado civil\")\n",
    "plt.ylabel(\"N¬∫ clientes\")\n",
    "plt.title('Distribuci√≥n de clientes por g√©nero y estado civil')\n",
    "plt.xticks(ticks=range(len(new_names)), labels=new_names, rotation=0)\n",
    "plt.grid(True, axis='y', color='#eeeeee')\n",
    "plt.legend(title=\"G√©nero\", labels=[\"Mujeres\", \"Hombres\"])\n",
    "plt.gca().spines[['right', \"top\"]].set_visible(False) # quitamos la l√≠nea de arriba y de la derecha\n",
    "plt.show()\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fda0e55",
   "metadata": {},
   "source": [
    "Respuesta: no hay diferencias rese√±ables entre g√©nero pero s√≠ entre estado civil. Son lso casados los que m√°s reservan seguidos de los solteros y en √∫ltimo caso los divorciados. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63726f56",
   "metadata": {},
   "source": [
    "#### BONUS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc58eb84",
   "metadata": {},
   "source": [
    "- Preparaci√≥n de Datos: Filtra el conjunto de datos para incluir √∫nicamente las columnas relevantes.   \n",
    "\n",
    "- An√°lisis Descriptivo: Agrupa los datos por nivel educativo y calcula estad√≠sticas descriptivas b√°sicas (como el promedio, la desviaci√≥n est√°ndar) del n√∫mero de vuelos eservados para cada grupo.  \n",
    "  \n",
    "- Prueba Estad√≠stica: Realiza una prueba de hip√≥tesis para determinar si existe una diferencia significativa en el n√∫mero de vuelos reservados entre los diferentes niveles educativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dbb317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Preparaci√≥n de datos: Generamos un nuevo df con las columnas indicadas: \n",
    "\n",
    "df_bon = df[[\"Flights Booked\", \"Education\"]]\n",
    "\n",
    "#  An√°lisis descriptivo: Agrupamos para visualizar las variables por nivel de educaci√≥n: \n",
    "\n",
    "df_bonus = df_bon.groupby(\"Education\")[\"Flights Booked\"].describe().T\n",
    "display(df_bonus)\n",
    "\n",
    "# Generamos una variable por cada nivel de educaci√≥n y mostramos s√≥lo la columna de vuelos reservados: \n",
    "\n",
    "bachelor_group = df_bon[df_bon[\"Education\"] == \"Bachelor\"][\"Flights Booked\"]\n",
    "college_group = df_bon[df_bon[\"Education\"] == \"College\"][\"Flights Booked\"]\n",
    "doctor_group = df_bon[df_bon[\"Education\"] == \"Doctor\"][\"Flights Booked\"]\n",
    "secondary_group = df_bon[df_bon[\"Education\"] == \"High School or Below\"][\"Flights Booked\"]\n",
    "master_group = df_bon[df_bon[\"Education\"] == \"Master\"][\"Flights Booked\"]\n",
    "\n",
    "# Prueba estad√≠stica: Activamos la funci√≥n de la prueba de hip√≥tesis adaptada para gestionar con varios grupos:\n",
    "\n",
    "multigroup_hypothesis_test(bachelor_group, college_group, doctor_group, secondary_group, master_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4002339e",
   "metadata": {},
   "source": [
    "Respuesta : los niveles educativos analizados no muestran diferencias significativas en la cantidad promedio de vuelos reservados, seg√∫n los datos y la prueba no param√©trica aplicada."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
